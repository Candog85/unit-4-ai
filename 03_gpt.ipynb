{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gpt_2_simple as gpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpt2.download_gpt2(\n",
    "#     model_name='124M'\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Variable model/wpe does not exist, or was not created with tf.get_variable(). Did you mean to set reuse=tf.AUTO_REUSE in VarScope?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m session\u001b[39m=\u001b[39mgpt2\u001b[39m.\u001b[39mstart_tf_sess()\n\u001b[0;32m----> 2\u001b[0m gpt2\u001b[39m.\u001b[39;49mload_gpt2(session, model_name\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m124M\u001b[39;49m\u001b[39m'\u001b[39;49m, reuse\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/Library/Python/3.10/lib/python/site-packages/gpt_2_simple/gpt_2.py:394\u001b[0m, in \u001b[0;36mload_gpt2\u001b[0;34m(sess, checkpoint, run_name, checkpoint_dir, model_name, model_dir, multi_gpu, reuse)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[39mif\u001b[39;00m multi_gpu:\n\u001b[1;32m    392\u001b[0m     gpus \u001b[39m=\u001b[39m get_available_gpus()\n\u001b[0;32m--> 394\u001b[0m output \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mmodel(hparams\u001b[39m=\u001b[39;49mhparams, X\u001b[39m=\u001b[39;49mcontext, gpus\u001b[39m=\u001b[39;49mgpus, reuse\u001b[39m=\u001b[39;49mreuse)\n\u001b[1;32m    396\u001b[0m \u001b[39mif\u001b[39;00m checkpoint\u001b[39m==\u001b[39m\u001b[39m'\u001b[39m\u001b[39mlatest\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    397\u001b[0m     ckpt \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mtrain\u001b[39m.\u001b[39mlatest_checkpoint(checkpoint_path)\n",
      "File \u001b[0;32m~/Library/Python/3.10/lib/python/site-packages/gpt_2_simple/src/model.py:188\u001b[0m, in \u001b[0;36mmodel\u001b[0;34m(hparams, X, past, scope, gpus, reuse)\u001b[0m\n\u001b[1;32m    185\u001b[0m results \u001b[39m=\u001b[39m {}\n\u001b[1;32m    186\u001b[0m batch, sequence \u001b[39m=\u001b[39m shape_list(X)\n\u001b[0;32m--> 188\u001b[0m wpe \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mcompat\u001b[39m.\u001b[39;49mv1\u001b[39m.\u001b[39;49mget_variable(\u001b[39m'\u001b[39;49m\u001b[39mwpe\u001b[39;49m\u001b[39m'\u001b[39;49m, [hparams\u001b[39m.\u001b[39;49mn_ctx, hparams\u001b[39m.\u001b[39;49mn_embd],\n\u001b[1;32m    189\u001b[0m                      initializer\u001b[39m=\u001b[39;49mtf\u001b[39m.\u001b[39;49mcompat\u001b[39m.\u001b[39;49mv1\u001b[39m.\u001b[39;49mrandom_normal_initializer(stddev\u001b[39m=\u001b[39;49m\u001b[39m0.01\u001b[39;49m))\n\u001b[1;32m    190\u001b[0m wte \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mcompat\u001b[39m.\u001b[39mv1\u001b[39m.\u001b[39mget_variable(\u001b[39m'\u001b[39m\u001b[39mwte\u001b[39m\u001b[39m'\u001b[39m, [hparams\u001b[39m.\u001b[39mn_vocab, hparams\u001b[39m.\u001b[39mn_embd],\n\u001b[1;32m    191\u001b[0m                      initializer\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mcompat\u001b[39m.\u001b[39mv1\u001b[39m.\u001b[39mrandom_normal_initializer(stddev\u001b[39m=\u001b[39m\u001b[39m0.02\u001b[39m))\n\u001b[1;32m    192\u001b[0m past_length \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m \u001b[39mif\u001b[39;00m past \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m tf\u001b[39m.\u001b[39mshape(\u001b[39minput\u001b[39m\u001b[39m=\u001b[39mpast)[\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m]\n",
      "File \u001b[0;32m~/Library/Python/3.10/lib/python/site-packages/tensorflow/python/ops/variable_scope.py:1565\u001b[0m, in \u001b[0;36mget_variable\u001b[0;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m   1549\u001b[0m \u001b[39m@tf_export\u001b[39m(v1\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mget_variable\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m   1550\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_variable\u001b[39m(name,\n\u001b[1;32m   1551\u001b[0m                  shape\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1563\u001b[0m                  synchronization\u001b[39m=\u001b[39mVariableSynchronization\u001b[39m.\u001b[39mAUTO,\n\u001b[1;32m   1564\u001b[0m                  aggregation\u001b[39m=\u001b[39mVariableAggregation\u001b[39m.\u001b[39mNONE):\n\u001b[0;32m-> 1565\u001b[0m   \u001b[39mreturn\u001b[39;00m get_variable_scope()\u001b[39m.\u001b[39;49mget_variable(\n\u001b[1;32m   1566\u001b[0m       _get_default_variable_store(),\n\u001b[1;32m   1567\u001b[0m       name,\n\u001b[1;32m   1568\u001b[0m       shape\u001b[39m=\u001b[39;49mshape,\n\u001b[1;32m   1569\u001b[0m       dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[1;32m   1570\u001b[0m       initializer\u001b[39m=\u001b[39;49minitializer,\n\u001b[1;32m   1571\u001b[0m       regularizer\u001b[39m=\u001b[39;49mregularizer,\n\u001b[1;32m   1572\u001b[0m       trainable\u001b[39m=\u001b[39;49mtrainable,\n\u001b[1;32m   1573\u001b[0m       collections\u001b[39m=\u001b[39;49mcollections,\n\u001b[1;32m   1574\u001b[0m       caching_device\u001b[39m=\u001b[39;49mcaching_device,\n\u001b[1;32m   1575\u001b[0m       partitioner\u001b[39m=\u001b[39;49mpartitioner,\n\u001b[1;32m   1576\u001b[0m       validate_shape\u001b[39m=\u001b[39;49mvalidate_shape,\n\u001b[1;32m   1577\u001b[0m       use_resource\u001b[39m=\u001b[39;49muse_resource,\n\u001b[1;32m   1578\u001b[0m       custom_getter\u001b[39m=\u001b[39;49mcustom_getter,\n\u001b[1;32m   1579\u001b[0m       constraint\u001b[39m=\u001b[39;49mconstraint,\n\u001b[1;32m   1580\u001b[0m       synchronization\u001b[39m=\u001b[39;49msynchronization,\n\u001b[1;32m   1581\u001b[0m       aggregation\u001b[39m=\u001b[39;49maggregation)\n",
      "File \u001b[0;32m~/Library/Python/3.10/lib/python/site-packages/tensorflow/python/ops/variable_scope.py:1275\u001b[0m, in \u001b[0;36mVariableScope.get_variable\u001b[0;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m   1273\u001b[0m \u001b[39mif\u001b[39;00m dtype \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1274\u001b[0m   dtype \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dtype\n\u001b[0;32m-> 1275\u001b[0m \u001b[39mreturn\u001b[39;00m var_store\u001b[39m.\u001b[39;49mget_variable(\n\u001b[1;32m   1276\u001b[0m     full_name,\n\u001b[1;32m   1277\u001b[0m     shape\u001b[39m=\u001b[39;49mshape,\n\u001b[1;32m   1278\u001b[0m     dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[1;32m   1279\u001b[0m     initializer\u001b[39m=\u001b[39;49minitializer,\n\u001b[1;32m   1280\u001b[0m     regularizer\u001b[39m=\u001b[39;49mregularizer,\n\u001b[1;32m   1281\u001b[0m     reuse\u001b[39m=\u001b[39;49mreuse,\n\u001b[1;32m   1282\u001b[0m     trainable\u001b[39m=\u001b[39;49mtrainable,\n\u001b[1;32m   1283\u001b[0m     collections\u001b[39m=\u001b[39;49mcollections,\n\u001b[1;32m   1284\u001b[0m     caching_device\u001b[39m=\u001b[39;49mcaching_device,\n\u001b[1;32m   1285\u001b[0m     partitioner\u001b[39m=\u001b[39;49mpartitioner,\n\u001b[1;32m   1286\u001b[0m     validate_shape\u001b[39m=\u001b[39;49mvalidate_shape,\n\u001b[1;32m   1287\u001b[0m     use_resource\u001b[39m=\u001b[39;49muse_resource,\n\u001b[1;32m   1288\u001b[0m     custom_getter\u001b[39m=\u001b[39;49mcustom_getter,\n\u001b[1;32m   1289\u001b[0m     constraint\u001b[39m=\u001b[39;49mconstraint,\n\u001b[1;32m   1290\u001b[0m     synchronization\u001b[39m=\u001b[39;49msynchronization,\n\u001b[1;32m   1291\u001b[0m     aggregation\u001b[39m=\u001b[39;49maggregation)\n",
      "File \u001b[0;32m~/Library/Python/3.10/lib/python/site-packages/tensorflow/python/ops/variable_scope.py:520\u001b[0m, in \u001b[0;36m_VariableStore.get_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    518\u001b[0m   \u001b[39mreturn\u001b[39;00m custom_getter(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcustom_getter_kwargs)\n\u001b[1;32m    519\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 520\u001b[0m   \u001b[39mreturn\u001b[39;00m _true_getter(\n\u001b[1;32m    521\u001b[0m       name,\n\u001b[1;32m    522\u001b[0m       shape\u001b[39m=\u001b[39;49mshape,\n\u001b[1;32m    523\u001b[0m       dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[1;32m    524\u001b[0m       initializer\u001b[39m=\u001b[39;49minitializer,\n\u001b[1;32m    525\u001b[0m       regularizer\u001b[39m=\u001b[39;49mregularizer,\n\u001b[1;32m    526\u001b[0m       reuse\u001b[39m=\u001b[39;49mreuse,\n\u001b[1;32m    527\u001b[0m       trainable\u001b[39m=\u001b[39;49mtrainable,\n\u001b[1;32m    528\u001b[0m       collections\u001b[39m=\u001b[39;49mcollections,\n\u001b[1;32m    529\u001b[0m       caching_device\u001b[39m=\u001b[39;49mcaching_device,\n\u001b[1;32m    530\u001b[0m       partitioner\u001b[39m=\u001b[39;49mpartitioner,\n\u001b[1;32m    531\u001b[0m       validate_shape\u001b[39m=\u001b[39;49mvalidate_shape,\n\u001b[1;32m    532\u001b[0m       use_resource\u001b[39m=\u001b[39;49muse_resource,\n\u001b[1;32m    533\u001b[0m       constraint\u001b[39m=\u001b[39;49mconstraint,\n\u001b[1;32m    534\u001b[0m       synchronization\u001b[39m=\u001b[39;49msynchronization,\n\u001b[1;32m    535\u001b[0m       aggregation\u001b[39m=\u001b[39;49maggregation)\n",
      "File \u001b[0;32m~/Library/Python/3.10/lib/python/site-packages/tensorflow/python/ops/variable_scope.py:473\u001b[0m, in \u001b[0;36m_VariableStore.get_variable.<locals>._true_getter\u001b[0;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    467\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m/part_0\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m name \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_vars:\n\u001b[1;32m    468\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    469\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mNo partitioner was provided, but a partitioned version of the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    470\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mvariable was found: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m/part_0. Perhaps a variable of the same \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    471\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mname was already created with partitioning?\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m name)\n\u001b[0;32m--> 473\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_single_variable(\n\u001b[1;32m    474\u001b[0m     name\u001b[39m=\u001b[39;49mname,\n\u001b[1;32m    475\u001b[0m     shape\u001b[39m=\u001b[39;49mshape,\n\u001b[1;32m    476\u001b[0m     dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[1;32m    477\u001b[0m     initializer\u001b[39m=\u001b[39;49minitializer,\n\u001b[1;32m    478\u001b[0m     regularizer\u001b[39m=\u001b[39;49mregularizer,\n\u001b[1;32m    479\u001b[0m     reuse\u001b[39m=\u001b[39;49mreuse,\n\u001b[1;32m    480\u001b[0m     trainable\u001b[39m=\u001b[39;49mtrainable,\n\u001b[1;32m    481\u001b[0m     collections\u001b[39m=\u001b[39;49mcollections,\n\u001b[1;32m    482\u001b[0m     caching_device\u001b[39m=\u001b[39;49mcaching_device,\n\u001b[1;32m    483\u001b[0m     validate_shape\u001b[39m=\u001b[39;49mvalidate_shape,\n\u001b[1;32m    484\u001b[0m     use_resource\u001b[39m=\u001b[39;49muse_resource,\n\u001b[1;32m    485\u001b[0m     constraint\u001b[39m=\u001b[39;49mconstraint,\n\u001b[1;32m    486\u001b[0m     synchronization\u001b[39m=\u001b[39;49msynchronization,\n\u001b[1;32m    487\u001b[0m     aggregation\u001b[39m=\u001b[39;49maggregation)\n",
      "File \u001b[0;32m~/Library/Python/3.10/lib/python/site-packages/tensorflow/python/ops/variable_scope.py:857\u001b[0m, in \u001b[0;36m_VariableStore._get_single_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    855\u001b[0m \u001b[39m# The code below handles only the case of creating a new variable.\u001b[39;00m\n\u001b[1;32m    856\u001b[0m \u001b[39mif\u001b[39;00m reuse \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 857\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mVariable \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m does not exist, or was not created with \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    858\u001b[0m                    \u001b[39m\"\u001b[39m\u001b[39mtf.get_variable(). Did you mean to set \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    859\u001b[0m                    \u001b[39m\"\u001b[39m\u001b[39mreuse=tf.AUTO_REUSE in VarScope?\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m name)\n\u001b[1;32m    861\u001b[0m \u001b[39m# Create the tensor to initialize the variable with default value.\u001b[39;00m\n\u001b[1;32m    862\u001b[0m \u001b[39mif\u001b[39;00m initializer \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: Variable model/wpe does not exist, or was not created with tf.get_variable(). Did you mean to set reuse=tf.AUTO_REUSE in VarScope?"
     ]
    }
   ],
   "source": [
    "session=gpt2.start_tf_sess()\n",
    "gpt2.load_gpt2(session, model_name='124M', reuse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There once a man from Nantuckett died in an accident.\n",
      "\n",
      "Two years ago a woman, who was a late bloomer, died in an accident.\n",
      "\n",
      "And Kansan is planning to take his own life during a campaign for the city's 12th ward.\n",
      "\n",
      "The city has 48 wards, including 11 that were built in 1912, and he says he does not care how many people die in a year.\n",
      "\n",
      "\"I don't care how many people are dead in a year,\" Kansan said. \"I've got a difference of opinion about that.\n",
      "\n",
      "\"The notion that someone can't do what they want to do is completely absurd.\"\n",
      "\n",
      "In recent years, the city has been doing more with less.\n",
      "\n",
      "A few years ago, a pregnant woman in her 60s, who was in the midst of having her second child, died from kidney failure.\n",
      "\n",
      "This year it was a woman with two children who had just met their second child.\n",
      "\n",
      "A man died in a car accident in 2015, but the city did not respond to a request for comment about his death.\n",
      "\n",
      "Still, the city is counting on people making long-term decisions about their lives.\n",
      "\n",
      "\"We are following the law,\" City Councilman Mike McAlpine said. \"We are following the law, and the ones who do it the best.\n",
      "\n",
      "\"You just have to be willing to do better.\"<|endoftext|>Page Content\n",
      "\n",
      "The Salt Lake City Police Department has processed over 100 complaints concerning the use of a stun gun in the past four years. (Photo: Utah Department of Public Safety/District of Columbia Police Department)\n",
      "\n",
      "The Salt Lake City Police Department has processed over 100 complaints concerning the use of a stun gun in the past four years.\n",
      "\n",
      "The agency has also announced that it's implementing a new policy which bans the use of stun guns in the city.\n",
      "\n",
      "But experts say that the new policy can also lead to unnecessary injuries.\n",
      "\n",
      "\"It's not just a short-term thing, but it can lead to a very serious injury,\" said Brian DeLong, an analyst with the American Civil Liberties Union of Utah.\n",
      "\n",
      "The policy also bans stun guns in public places, including police stations, parks, playgrounds and other public places. When the new policy goes into effect, the law would have to be amended to prohibit the use of the stun gun in a place where there is a delay in the police responding to a call.\n",
      "\n",
      "The policy also bans the use of stun guns in a public place if the person is under the age of 20.\n",
      "\n",
      "While it's possible that some people will not be able to use the stun gun in the next few months, it's unlikely that they will.\n",
      "\n",
      "\"It's too early to see how that will impact on the community,\" said DeLong. \"I think it's frustrating to see so many people being shut out of the community for some reason, and that's frustrating for many people. It's sad.\"\n",
      "\n",
      "DeLong said that the policy is not intended to eliminate the harm that stun guns can cause.\n",
      "\n",
      "\"It's not about making it legal,\" he said. \"It's about making it legal for everyone to use them. It's about making it legal for every person, regardless of age or disability, to use them.\"\n",
      "\n",
      "The law that has been in effect since January was that the public could not carry a stun gun in a public place as long as the person did not have a valid discharge permit.\n",
      "\n",
      "The new policy, which was introduced last month, bans the use of the stun gun in a public place if the person is under the age of 20.\n",
      "\n",
      "The new policy also bans the use of a stun gun in a park.\n",
      "\n",
      "Police officers can't use stun guns in public places but can use them on their own, DeLong said.\n",
      "\n",
      "\"It's a violation of the ordinance that they make a request for somebody to use a stun gun,\" DeLong said. \"It's not a restriction on the use of a stun gun.\"\n",
      "\n",
      "The policy also prohibits the use of a stun gun in a public place if the person is under the age of 20.\n",
      "\n",
      "The policy does not impose a penalty.\n",
      "\n",
      "The ACLU of Utah also is fighting the policy.\n",
      "\n",
      "\"The city should have the option of taking a stand,\" said Leah Stojnyagnak, a staff attorney with the ACLU of Utah. \"If the police department wants to use a stun gun, then they have to comply with the policy. If they don't, then they're in the wrong.\"\n",
      "\n",
      "Stojnyagnak also said that the ban should be a priority for the city because it would provide a better opportunity for people with disabilities to use a stun gun.\n",
      "\n",
      "\"It's a public health issue,\" she said. \"It's a health issue, and it's just not going to go away.\"\n"
     ]
    }
   ],
   "source": [
    "gpt2.generate(\n",
    "    session,\n",
    "    model_name='124M',\n",
    "    prefix='There once a man from Nantuckett'\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shakespeare GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "session2= gpt2.start_tf_sess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1732547135.867352 2179127 mlir_graph_optimization_pass.cc:401] MLIR V1 optimization pass is not enabled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint models/124M/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/124M/model.ckpt\n",
      "Loading dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset has 338025 tokens\n",
      "Training...\n",
      "[1 | 19.70] loss=4.01 avg=4.01\n",
      "[2 | 40.99] loss=3.74 avg=3.87\n",
      "[3 | 61.23] loss=3.53 avg=3.76\n",
      "[4 | 81.17] loss=3.67 avg=3.74\n",
      "[5 | 101.76] loss=3.72 avg=3.73\n",
      "[6 | 121.88] loss=3.81 avg=3.75\n",
      "[7 | 142.33] loss=3.66 avg=3.73\n",
      "[8 | 162.52] loss=3.73 avg=3.73\n",
      "[9 | 183.28] loss=3.71 avg=3.73\n",
      "[10 | 203.34] loss=3.64 avg=3.72\n",
      "[11 | 224.09] loss=3.40 avg=3.69\n",
      "[12 | 243.92] loss=3.62 avg=3.68\n",
      "[13 | 263.45] loss=3.33 avg=3.65\n",
      "[14 | 283.01] loss=3.65 avg=3.65\n",
      "[15 | 302.39] loss=3.43 avg=3.64\n",
      "[16 | 321.60] loss=3.31 avg=3.62\n",
      "[17 | 341.02] loss=3.44 avg=3.60\n",
      "[18 | 360.99] loss=3.52 avg=3.60\n",
      "[19 | 380.85] loss=3.56 avg=3.60\n",
      "[20 | 400.60] loss=3.55 avg=3.59\n",
      "[21 | 420.75] loss=3.57 avg=3.59\n",
      "[22 | 441.93] loss=3.22 avg=3.57\n",
      "[23 | 461.68] loss=3.33 avg=3.56\n",
      "[24 | 481.75] loss=3.62 avg=3.56\n",
      "[25 | 502.06] loss=3.25 avg=3.55\n",
      "[26 | 521.99] loss=3.57 avg=3.55\n",
      "[27 | 541.25] loss=3.52 avg=3.55\n",
      "[28 | 561.34] loss=3.62 avg=3.55\n",
      "[29 | 581.11] loss=3.41 avg=3.55\n",
      "[30 | 600.82] loss=3.43 avg=3.54\n",
      "[31 | 620.44] loss=3.46 avg=3.54\n",
      "[32 | 640.08] loss=3.28 avg=3.53\n",
      "[33 | 659.68] loss=3.25 avg=3.52\n",
      "[34 | 678.62] loss=3.11 avg=3.51\n",
      "[35 | 698.60] loss=3.34 avg=3.50\n",
      "[36 | 720.44] loss=3.25 avg=3.49\n",
      "[37 | 742.37] loss=3.37 avg=3.49\n",
      "[38 | 763.86] loss=3.40 avg=3.49\n",
      "[39 | 784.56] loss=3.43 avg=3.48\n",
      "[40 | 804.93] loss=3.34 avg=3.48\n",
      "[41 | 825.61] loss=3.29 avg=3.47\n",
      "[42 | 846.06] loss=3.29 avg=3.47\n",
      "[43 | 867.36] loss=3.53 avg=3.47\n",
      "[44 | 887.67] loss=3.20 avg=3.46\n",
      "[45 | 908.23] loss=3.46 avg=3.46\n",
      "[46 | 928.31] loss=3.23 avg=3.46\n",
      "[47 | 948.63] loss=3.31 avg=3.45\n",
      "[48 | 969.64] loss=3.37 avg=3.45\n",
      "[49 | 990.19] loss=2.98 avg=3.44\n",
      "[50 | 1012.26] loss=2.99 avg=3.43\n",
      "[51 | 1032.14] loss=3.31 avg=3.42\n",
      "[52 | 1051.88] loss=3.40 avg=3.42\n",
      "[53 | 1073.92] loss=3.26 avg=3.42\n",
      "[54 | 1094.38] loss=3.28 avg=3.42\n",
      "[55 | 1115.15] loss=2.99 avg=3.41\n",
      "[56 | 1134.83] loss=3.39 avg=3.41\n",
      "[57 | 1153.50] loss=3.36 avg=3.40\n",
      "[58 | 1173.30] loss=3.41 avg=3.40\n",
      "[59 | 1193.14] loss=3.31 avg=3.40\n",
      "[60 | 1212.73] loss=3.38 avg=3.40\n",
      "[61 | 1232.74] loss=3.15 avg=3.40\n",
      "[62 | 1253.69] loss=3.10 avg=3.39\n",
      "[63 | 1273.62] loss=3.35 avg=3.39\n",
      "[64 | 1293.43] loss=3.25 avg=3.39\n",
      "[65 | 1313.44] loss=3.41 avg=3.39\n",
      "[66 | 1333.26] loss=3.38 avg=3.39\n",
      "[67 | 1355.20] loss=3.30 avg=3.39\n",
      "[68 | 1376.22] loss=3.35 avg=3.38\n",
      "[69 | 1396.56] loss=3.26 avg=3.38\n",
      "[70 | 1416.67] loss=3.32 avg=3.38\n",
      "[71 | 1437.15] loss=3.19 avg=3.38\n",
      "[72 | 1456.73] loss=3.37 avg=3.38\n",
      "[73 | 1476.97] loss=3.29 avg=3.38\n",
      "[74 | 1496.96] loss=3.10 avg=3.37\n",
      "[75 | 1516.77] loss=3.17 avg=3.37\n",
      "[76 | 1537.32] loss=3.20 avg=3.36\n",
      "[77 | 1557.77] loss=3.06 avg=3.36\n",
      "[78 | 1578.70] loss=3.09 avg=3.35\n",
      "[79 | 1599.05] loss=3.40 avg=3.35\n",
      "[80 | 1618.58] loss=2.94 avg=3.35\n",
      "[81 | 1637.74] loss=3.25 avg=3.34\n",
      "[82 | 1655.71] loss=2.95 avg=3.34\n",
      "[83 | 1673.91] loss=3.19 avg=3.33\n",
      "[84 | 1691.74] loss=3.36 avg=3.34\n",
      "[85 | 1709.78] loss=3.24 avg=3.33\n",
      "[86 | 1727.84] loss=3.10 avg=3.33\n",
      "[87 | 1745.46] loss=3.09 avg=3.33\n",
      "[88 | 1764.03] loss=3.33 avg=3.33\n",
      "[89 | 1784.24] loss=3.12 avg=3.32\n",
      "[90 | 1803.84] loss=2.89 avg=3.31\n",
      "[91 | 1822.01] loss=3.14 avg=3.31\n",
      "[92 | 1841.92] loss=3.13 avg=3.31\n",
      "[93 | 1861.14] loss=3.10 avg=3.31\n",
      "[94 | 1879.94] loss=3.08 avg=3.30\n",
      "[95 | 1898.10] loss=3.07 avg=3.30\n",
      "[96 | 1917.51] loss=3.22 avg=3.30\n",
      "[97 | 1935.49] loss=3.22 avg=3.30\n",
      "[98 | 1953.96] loss=3.03 avg=3.29\n",
      "[99 | 1971.79] loss=3.26 avg=3.29\n",
      "[100 | 1990.22] loss=3.19 avg=3.29\n",
      "Saving checkpoint/shakespeare/model-100\n"
     ]
    }
   ],
   "source": [
    "gpt2.finetune(\n",
    "    session2,\n",
    "    'shakespeare.txt',\n",
    "    model_name='124M',\n",
    "    steps=100,\n",
    "    run_name='shakespeare'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kerdon: By the gods! Skibidi toiletus?\n",
      "\n",
      "LADY MARIANA:\n",
      "Amen.\n",
      "\n",
      "BENVOLIO:\n",
      "I thank the gods!\n",
      "\n",
      "MENENIUS:\n",
      "And we, to our great good fortune,\n",
      "To bear the battle with you.\n",
      "\n",
      "BENVOLIO:\n",
      "The gods of Tybalt,\n",
      "We will not be long in front\n",
      "Of the time-honoured day.\n",
      "\n",
      "BENVOLIO:\n",
      "The gods of Tybalt!\n",
      "\n",
      "MENENIUS:\n",
      "Relax, the gods of Tybalt!\n",
      "\n",
      "BENVOLIO:\n",
      "O, the gods of Tybalt!\n",
      "\n",
      "LADY MARIANA:\n",
      "The gods of Tybalt!\n",
      "\n",
      "BENVOLIO:\n",
      "The gods of Tybalt!\n",
      "\n",
      "MENENIUS:\n",
      "The gods of Tybalt!\n",
      "\n",
      "BENVOLIO:\n",
      "The gods of Tybalt!\n",
      "\n",
      "MENENIUS:\n",
      "The gods of Tybalt!\n",
      "\n",
      "BENVOLIO:\n",
      "The gods of Tybalt!\n",
      "\n",
      "MENENIUS:\n",
      "The gods of Tybalt!\n",
      "\n",
      "BENVOLIO:\n",
      "The gods of Tybalt!\n",
      "\n",
      "MENENIUS:\n",
      "The gods of Tybalt!\n",
      "\n",
      "BENVOLIO:\n",
      "The gods of Tybalt!\n",
      "\n",
      "MENENIUS:\n",
      "The gods of Tybalt!\n",
      "\n",
      "BENVOLIO:\n",
      "The gods of Tybalt!\n",
      "\n",
      "MENENIUS:\n",
      "The gods of Tybalt!\n",
      "\n",
      "BENVOLIO:\n",
      "The gods of Tybalt!\n",
      "\n",
      "MENENIUS:\n",
      "The gods of Tybalt!\n",
      "\n",
      "BENVOLIO:\n",
      "The gods of Tybalt!\n",
      "\n",
      "MENENIUS:\n",
      "The gods of Tybalt!\n",
      "\n",
      "BENVOLIO:\n",
      "The gods of Tybalt!\n",
      "\n",
      "MENENIUS:\n",
      "The gods of Tybalt!\n",
      "\n",
      "BENVOLIO:\n",
      "The gods of Tybalt!\n",
      "\n",
      "MENENIUS:\n",
      "The gods of Tybalt!\n",
      "\n",
      "BENVOLIO:\n",
      "The gods of Tybalt!\n",
      "\n",
      "MENENIUS:\n",
      "The gods of Tybalt!\n",
      "\n",
      "BENVOLIO:\n",
      "The gods of Tybalt!\n",
      "\n",
      "MENENIUS:\n",
      "The gods of Tybalt!\n",
      "\n",
      "BENVOLIO:\n",
      "The gods of Tybalt!\n",
      "\n",
      "MENENIUS:\n",
      "The gods of Tybalt!\n",
      "\n",
      "BENVOLIO:\n",
      "The gods of Tybalt!\n",
      "\n",
      "MENENIUS:\n",
      "The gods of Tybalt!\n",
      "\n",
      "BENVOLIO:\n",
      "The gods of Tybalt!\n",
      "\n",
      "MENENIUS:\n",
      "The gods of Tybalt!\n",
      "\n",
      "BENVOLIO:\n",
      "The gods of Tybalt!\n",
      "\n",
      "MENENIUS:\n",
      "The gods of Tybalt!\n",
      "\n",
      "BENVOLIO:\n",
      "The gods of Tybalt!\n",
      "\n",
      "MENENIUS:\n",
      "The gods of Tybalt!\n",
      "\n",
      "BENVOLIO:\n",
      "The gods of Tybalt!\n",
      "\n",
      "MENENIUS:\n",
      "The gods of Tybalt!\n",
      "\n",
      "BENVOLIO:\n",
      "The gods of Tybalt!\n",
      "\n",
      "MENENIUS:\n",
      "The gods of Tybalt!\n",
      "\n",
      "BENVOLIO:\n",
      "The gods of Tybalt!\n",
      "\n",
      "MENENIUS:\n",
      "The gods of Tybalt!\n",
      "\n",
      "BENVOLIO:\n",
      "The gods of Tybalt!\n",
      "\n",
      "MENENIUS:\n",
      "The gods of Tybalt!\n",
      "\n",
      "BENVOLIO:\n",
      "The gods of Tybalt!\n",
      "\n",
      "MENENIUS:\n",
      "The gods of Tybalt!\n",
      "\n",
      "BENVOLIO:\n",
      "The gods of Tybalt!\n",
      "\n",
      "MENENIUS:\n",
      "The gods of Tybalt!\n",
      "\n",
      "BENVOLIO:\n",
      "The gods of Tybalt!\n",
      "\n",
      "MENENIUS:\n",
      "The gods of Tybalt!\n",
      "\n",
      "BENVOLIO:\n",
      "The gods of Tybalt!\n",
      "\n",
      "MENENIUS:\n",
      "The gods of Tybalt!\n",
      "\n",
      "BENVOLIO:\n",
      "The gods of Tybalt!\n",
      "\n",
      "MENENIUS:\n",
      "The gods of Tybalt!\n",
      "\n",
      "BENVOLIO:\n",
      "\n",
      "MENENIUS:\n",
      "The gods of Tybalt!\n",
      "\n",
      "BENVOLIO:\n",
      "The gods of Tybalt!\n",
      "\n",
      "MENENIUS:\n",
      "The gods of Tybalt!\n",
      "\n",
      "BENVOLIO:\n",
      "The gods of Tybalt!\n",
      "\n",
      "MENENIUS:\n",
      "The gods of Ty\n"
     ]
    }
   ],
   "source": [
    "gpt2.generate(\n",
    "    session2,\n",
    "    prefix='Kerdon: By the gods! Skibidi toiletus?',\n",
    "    run_name='shakespeare'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
